1. 提出一种非对称的 编码解码器。
2. 是一种自监督的学习任务
3. 在遮掩高比例的图像区域时，效果惊人
4. 此模型具有很高的拓展性

## 介绍

随着深度学习技术在图像领域的进一步发展，训练模型模型的成本进一步提高，需要大量的已标记图像。BERT在NLP领域大量应用，其基本方法其实很简单：一处数据的一部分并学习预测移除的内容。但是视觉自动编码在图像领域并不常见。

BERT模型 ---- MAE模型


NLP自动掩码和图像自动掩码之间的不同：

1. 模型架构的不同 ---- 传统CNN网络 通过位置嵌入指示符等方式实现掩码的效果并不简单。但是随着VIT的提出，模型结构趋于一统，使用VIT进行自动掩码就没有什么阻碍
图像
2. 连个的信息密度不同。语言具有高信息密集性，相反图像具有重空间冗余的自然信号。为了克服冗余，我们随机掩盖较大比例的图像75%。
图像

模型总体的思想：
1. 对输入图像进行随机掩码 
2. 在像素空间中重建掩码区域

3. 使用不对称的编码-解码器设置

编码器只训练为掩码的区域，编码后的可见区域和掩码区域一同放入解码器用于图像重建。

同时此模型具有很好的泛化能力，在IMAGENET1K上 使用ViT-Large/-Huge这种需要大量数据的模型进行自监督训练，实现了87.8的准确率，同时在语义分割和实例分割中具有不错的效果。




